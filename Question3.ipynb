{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d884484f",
   "metadata": {},
   "source": [
    "# Libraries and Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fb600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use ! \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os \n",
    "import collections\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import streamlit as st\n",
    "from traitlets.config import Config\n",
    "from itertools import compress\n",
    "\n",
    "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "dir = 'Data Mining Assignment Dataset'\n",
    "# if u want to show graph in this note pls change to true (this wll make streamlit having error if convert with True)\n",
    "show_output_on_notebook = False\n",
    "\n",
    "print(\"Ready to use ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80f07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 18:00:01.177 WARNING root: \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\wangl\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "QuestionSB = st.sidebar.selectbox(\"Questions\", [\"Question 1(Missing Value)\", \"Question 1(EDA)\", \"Question 2\", \"Question 3\", \"Question 4\"], key=\"QuestionSB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102580a7",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fe16a",
   "metadata": {},
   "source": [
    "# List of Dataframe and States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdaca82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(dir+\"/*.\" +\"csv\")\n",
    "if show_output_on_notebook:\n",
    "    print(file_list)\n",
    "\n",
    "df = {}\n",
    "for i, names in enumerate(file_list):\n",
    "    df[file_list[i].replace(dir+\"\\\\\", '').replace(\".csv\",'')] = pd.read_csv(file_list[i])\n",
    "if show_output_on_notebook:\n",
    "    print(df['deaths_state'].head())\n",
    "\n",
    "# Get the array list for States in Malaysia\n",
    "state_list = list(np.unique(df['deaths_state']['state']))\n",
    "if show_output_on_notebook:\n",
    "    print(state_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ed27",
   "metadata": {},
   "source": [
    "# Question (i) Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce1c42",
   "metadata": {},
   "source": [
    "## EDA and Outliers detection\n",
    "\n",
    "#### For EDA, I will be using pd.Dataframe.describe() function to get the summary of the dataframe, such as the number of rows, 5 number summary, etc. This will give me a better feel and understanding of the data properties.\n",
    "\n",
    "####  Because of the nature of the data, I will be setting the outlier boundary with the formula: Q3 + 3IQR and not Q1-3IQR because the data started really low and it is impossible to get negative cases infected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae228f",
   "metadata": {},
   "source": [
    "## Missing Values Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716574e5",
   "metadata": {},
   "source": [
    "#### Unlike looking for outliers, which might require different detection methods depending on the nature of the data, it is always good to know all the locations of missing values in the datasets. \n",
    "#### In this part of the code, I will be looping through every row for each column in all of the datasets in this assignment. The information of data missing including the type of data, location and the number of missing values would be detected in this following codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d786129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in df.items():\n",
    "    date_columns = [column for column in value.columns if 'date'in column]\n",
    "    if len(date_columns) > 0:\n",
    "        for date_column in date_columns:\n",
    "            df[key][date_column] = pd.to_datetime(df[key][date_column])\n",
    "        \n",
    "if QuestionSB == \"Question 1(Missing Value)\":\n",
    "    st.title(\"Data Type Checking and Outliers Detection\")\n",
    "    for key, value in df.items():\n",
    "        st.header(\"Dataframe/File Name: \"+key)\n",
    "        variable_dtypes_df = value.dtypes.to_frame()\n",
    "        variable_dtypes_df.rename(columns={0: \"dtypes\"}, inplace=True)\n",
    "        variable_dtypes_df[\"isna\"] = value.isna().sum()\n",
    "        st.write(variable_dtypes_df)\n",
    "        if show_output_on_notebook:\n",
    "            print(variable_dtypes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401c26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanceGrouping(df,alignColumn,targetColumn):\n",
    "    cases_states_df = pd.DataFrame(data={alignColumn: df[alignColumn].unique()})\n",
    "    for value in df[targetColumn].unique():\n",
    "        cases_state_df = df[df[targetColumn] == value]\n",
    "        cases_state_df.rename(columns={'cases_new': value}, inplace=True)\n",
    "        cases_state_df.drop([targetColumn], axis='columns', inplace=True)\n",
    "        cases_states_df = cases_states_df.merge(cases_state_df, how='inner', on='date')\n",
    "    return cases_states_df\n",
    "\n",
    "def createBoxPlot(df,show_count_table, show_output_on_notebook):\n",
    "    sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "    ax = sns.boxplot(data=df)\n",
    "    if(df.shape[1] > 1):\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation=-60, horizontalalignment='left')\n",
    "    if show_output_on_notebook:\n",
    "        plt.show(ax)\n",
    "    st.pyplot()\n",
    "\n",
    "    Q1 = df.quantile(.25)\n",
    "    Q3 = df.quantile(.75)\n",
    "    IQR = Q3 - Q1\n",
    "    cases_outliers_df = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum().to_frame()\n",
    "    cases_outliers_df.rename(columns={0: \"Count of outlier\"}, inplace=True)\n",
    "    if show_count_table:\n",
    "        st.table(cases_outliers_df)\n",
    "    if show_output_on_notebook:\n",
    "        print(cases_outliers_df)\n",
    "    return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "if QuestionSB == \"Question 1(EDA)\" or show_output_on_notebook:\n",
    "    st.title(\"Exploratory Data Analysis\")\n",
    "    state_listSB = st.selectbox(\"State\", [\"All State\"]+state_list)\n",
    "    \n",
    "    cases_states_df = advanceGrouping(df['cases_state'],'date','state')\n",
    "    if(state_listSB != \"All State\"):\n",
    "        state = state_listSB\n",
    "        \n",
    "        st.header(state + \"'s cases_state EDA\")\n",
    "        st.table(cases_states_df[state].to_frame().describe())\n",
    "        \n",
    "        st.header(state + \"'s boxplot\")\n",
    "        mask = createBoxPlot(cases_states_df[state].to_frame(),False, show_output_on_notebook)\n",
    "        \n",
    "        st.header(state + \"'s outlier\")\n",
    "        outlier_df = pd.DataFrame(list(compress(cases_states_df[state], mask[state])))\n",
    "        outlier_df.rename(columns={0: \"outliers\"}, inplace=True)\n",
    "        st.table(outlier_df)\n",
    "    else:\n",
    "        st.header(\"All state's cases_state EDA\")\n",
    "        st.write(cases_states_df.describe())\n",
    "        print(cases_states_df.describe())\n",
    "        \n",
    "        st.header(\"All state's boxplot\")\n",
    "        createBoxPlot(cases_states_df[cases_states_df.columns.difference(['date'])],True, show_output_on_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35c00c",
   "metadata": {},
   "source": [
    "# Question (ii) Answers\n",
    "\n",
    "## Question: What are the states that exhibit strong correlation with Pahang and Johor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f268746",
   "metadata": {},
   "source": [
    "#### To search for the states that has strong correlation with Pahang and Johor, I will be using 'corr()' function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e2269c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Joining the states into a single dataframe \n",
    "if QuestionSB == \"Question 2\"  or show_output_on_notebook:\n",
    "    st.title(\"Correlation\")\n",
    "    st.header(\"Heat Map of case state\")\n",
    "    state_cases_df = advanceGrouping(df['cases_state'],'date','state')\n",
    "    state_cases_correlation = state_cases_df.corr()\n",
    "    ax = sns.heatmap(state_cases_correlation, vmax=.8, square=True, annot=True, fmt='.2f', annot_kws={'size': 5}, cmap=sns.color_palette(\"Blues\"))\n",
    "    if show_output_on_notebook:\n",
    "        plt.show(ax)\n",
    "    st.pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8250aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QuestionSB == \"Question 2\"  or show_output_on_notebook:\n",
    "    pahang_correlation = state_cases_correlation.loc[state_cases_correlation.index == 'Pahang']\n",
    "    johor_correlation = state_cases_correlation.loc[state_cases_correlation.index == 'Johor']\n",
    "    \n",
    "    st.header(\"Heat Map of case state (Johor)\")\n",
    "    ax = johor_correlation.plot.bar(rot=0, figsize = (15,8), title = 'Correlation with Johor')\n",
    "    if show_output_on_notebook:\n",
    "        plt.show()\n",
    "    st.pyplot()\n",
    "    \n",
    "    st.header(\"Heat Map of case state (Pahang)\")\n",
    "    ax1 = pahang_correlation.plot.bar(rot=0, figsize = (15,8), title = 'Correlation with Pahang')\n",
    "    if show_output_on_notebook:\n",
    "        plt.show()\n",
    "    st.pyplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae5af6",
   "metadata": {},
   "source": [
    "### From the correlation table, we could see that the state Pahang is strongly correlated with Kedah, Melaka, Selangor and Terrenganu. These for states posted a correlation at a rate of above 90% (0.9)\n",
    "\n",
    "### From the correlation table, we could see that none of the states have a correlation with Johor that is abouve 90%(0.9). The state with the highest correlation with Johor is Pulau Pinang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515466d",
   "metadata": {},
   "source": [
    "# Question (iii) Answers\n",
    "\n",
    "### What are the strong features/indicators to daily cases for Pahang, Kedah, Johor and Selangor. Use at least 2 methods to justify your findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6736e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part of the code will show error if it is runned for the second time without re-runing the whole program \n",
    "\n",
    "# Get all the individual state of the question from the dataframe \n",
    "q3_state_list = ['Johor', 'Pahang', 'Kedah', 'Selangor']\n",
    "\n",
    "q3_df_title = [file.replace(dir+\"\\\\\", '').replace(\".csv\",'') for file in file_list]\n",
    "    \n",
    "q3_df_title.remove('clusters')\n",
    "q3_df_title.remove('population')\n",
    "q3_df_title.remove('cases_malaysia')\n",
    "q3_df_title.remove('deaths_malaysia')\n",
    "if show_output_on_notebook:\n",
    "    print(q3_df_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896ab082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangl\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9186: FutureWarning: Passing 'suffixes' which cause duplicate columns {'state_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return merge(\n"
     ]
    }
   ],
   "source": [
    "q3_combined_df = {}\n",
    "\n",
    "# Isolate individual states into seperate dataframes and combining all the data collected except for clusters.csv and population.csv and country level data\n",
    "for q3_state in q3_state_list:\n",
    "    i = 0\n",
    "    for q3_title in q3_df_title:\n",
    "        q3_df = df[q3_title]\n",
    "        if show_output_on_notebook:\n",
    "            print('Currently Extracting ' + q3_title +' from '+ q3_state)\n",
    "        q3_temp_df = q3_df[q3_df['state'] == q3_state]\n",
    "        if i == 0: \n",
    "            q3_combined_df[q3_state] = q3_temp_df\n",
    "        else:\n",
    "            q3_combined_df[q3_state] = q3_combined_df[q3_state].merge(q3_temp_df, how='inner', on='date')\n",
    "        i+=1\n",
    "        \n",
    "# Remove the duplicated 'state' column in the dataframes\n",
    "for states in q3_state_list:\n",
    "    q3_combined_df[states] = q3_combined_df[states].drop(columns=['state_x', 'state_y'])\n",
    "    q3_combined_df[states]['state'] = states\n",
    "\n",
    "q3_combined_df = q3_combined_df\n",
    "if show_output_on_notebook:\n",
    "    print(q3_combined_df[\"Johor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83214bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_corr_features = []\n",
    "Question3GraphType = \"\"\n",
    "if QuestionSB == \"Question 3\"  or show_output_on_notebook:\n",
    "    Question3GraphType = st.selectbox(\"Graph Type\", [\"Heat Map\",\"Bar Chart\"], key=\"Question 3 Graph Type\")\n",
    "    st.title(Question3GraphType + \" to show Strong Features/Indicators to Daily Cases\")\n",
    "\n",
    "for corr_states in q3_state_list:\n",
    "    if show_output_on_notebook:\n",
    "        print('-'*60)\n",
    "        print('Currently Showing state: '+ corr_states)\n",
    "    q3_corr_df = q3_combined_df[corr_states]\n",
    "#     q3_corr_df = q3_corr_df.fillna(0)\n",
    "    q3_corr_df = q3_corr_df.drop(columns=['state'])\n",
    "    q3_corr_df = q3_corr_df.corr()\n",
    "    q3_cases_corr_df = q3_corr_df.loc[q3_corr_df.index == 'cases_new']\n",
    "    # Ploting the heatmap and shows the features that has a correlation higher than 0.7\n",
    "    plt.figure(figsize=(22,2))\n",
    "    ax = sns.heatmap(q3_cases_corr_df, vmin=-1, vmax=1, cbar=False,\n",
    "                     cmap='coolwarm', annot=True)\n",
    "    # Set the condition so that only features with 0.7 or higher correlation will show \n",
    "    for text in ax.texts:\n",
    "        t = float(text.get_text())\n",
    "        if -0.7 < t < 0.7:\n",
    "            text.set_text('')\n",
    "        else:\n",
    "            text.set_text(round(t, 2))\n",
    "        text.set_fontsize('x-large')\n",
    "    plt.xticks( size='x-large')\n",
    "    plt.yticks(rotation=0, size='x-large')\n",
    "    if show_output_on_notebook:\n",
    "        plt.show()\n",
    "    if QuestionSB == \"Question 3\" and Question3GraphType == \"Heat Map\":\n",
    "        st.header(\"Currently Showing state: \" + corr_states)\n",
    "        st.pyplot()\n",
    "    strong_features = q3_corr_df['cases_new']\n",
    "    strong_features = strong_features.loc[strong_features > 0.75]\n",
    "    strong_features_list = list(strong_features.index)\n",
    "    for corr_feat in strong_features_list:\n",
    "        top_corr_features.append(corr_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad2c3b",
   "metadata": {},
   "source": [
    "### For the first method, we have decided to use pearson correlation to check what features has a stronger cause and effect relationship with the daily cases for each different state individually. We then select the commonly shared features that has a correlation of above 0.7 to justify that the feature selected is universally applied to each of these states. \n",
    "\n",
    "### The commonlly shared features that has a correlation score higher than 0.7 is beds_icu_rep,  beds_icu_total, beds_icu_covid, icu_covid, vent_covid, vent_noncovid, admitted_covid, admitted_total, discharged_covid, discharged_total and hosp_covid. These features are the ones that will be the most helpful for training a prediction model based on pearsons correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11bff026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Method Regression Feature Selection\n",
    "\n",
    "# This set the number of the most relevant features\n",
    "top_mutual_features = []\n",
    "threshold = 10\n",
    "\n",
    "for filter_states in q3_state_list:\n",
    "    if show_output_on_notebook:\n",
    "        print('-'*60)\n",
    "        print('Currently Showing state: '+ filter_states)\n",
    "    q3_filter_df = q3_combined_df[filter_states]\n",
    "    y = q3_filter_df['cases_new']\n",
    "    X = q3_filter_df.drop(columns=['state', 'cases_new', 'date'])\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    # Plot a barchart for easier visualization \n",
    "    feature_score = mutual_info_classif(X,y)\n",
    "    feat_importances = pd.Series(feature_score, X.columns)\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    feat_importances.plot(kind='bar')\n",
    "    if show_output_on_notebook:\n",
    "        plt.show()\n",
    "    if QuestionSB == \"Question 3\" and Question3GraphType == \"Bar Chart\":\n",
    "        st.header(\"Currently Showing state: \" + filter_states)\n",
    "        st.pyplot()\n",
    "\n",
    "    # Display the top 10 most relevant features based on mutual info classification \n",
    "    if show_output_on_notebook:\n",
    "        print('Top 10 most relevant')\n",
    "    if QuestionSB == \"Question 3\" and Question3GraphType == \"Bar Chart\":\n",
    "        st.subheader('Top 10 most relevant')\n",
    "    for score, f_name in sorted(zip(feature_score, X.columns), reverse=True)[:threshold]:\n",
    "        if QuestionSB == \"Question 3\" and Question3GraphType == \"Bar Chart\":\n",
    "            st.markdown(f_name + \":\" + str(round(score, 2)))\n",
    "        if show_output_on_notebook:\n",
    "            print(f_name, round(score, 2)) \n",
    "        top_mutual_features.append(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41c332",
   "metadata": {},
   "source": [
    "### For the second method, we have chosen a module form sklearn which is the mutual info classification technique., which measure the dependency between the variables. The higher the values, the higher the dependency\n",
    "\n",
    "### From the results, we could see many features appear as the top 10 relevant in all four states, such as vents, bed_icu etc. The appearance of these features in all 4 states top 10 relevant features might suggest that these are more universal indicators that represent strong correlation with daily cases. In other words, these features might generalize better when it is used to predict other state's daily cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d57d68",
   "metadata": {},
   "source": [
    "# Question (iv) Answers\n",
    "\n",
    "### Comparing regression and classification models, what model performs well in predicting the daily cases for Pahang, Kedah, Johor and Selangor? \n",
    "### Requirement, use two regression and classification models and use the appropriate evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f53817",
   "metadata": {},
   "source": [
    "### For the data used to train the classification model for daily cases forecasting, we decide to use only the features that shows high correlation and dependancy with daily cases from question 3 above. \n",
    "\n",
    "### Starting off, we will extract the features of columns that shows high universal correlation and dependencies in question 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466c7414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if QuestionSB == \"Question 4\"  or show_output_on_notebook:\n",
    "    # Get the repeated features in mutual_info_class\n",
    "    mutual_word_counts = collections.Counter(top_mutual_features)\n",
    "    repeated_mutual_features = []\n",
    "\n",
    "    for mutual_word, mutual_count in sorted(mutual_word_counts.items()):\n",
    "        if show_output_on_notebook:\n",
    "            print('\"%s\" is repeated %d time%s.' % (mutual_word, mutual_count, \"s\" if mutual_count > 1 else \"\"))\n",
    "        if mutual_count >= 3:\n",
    "            repeated_mutual_features.append(mutual_word)\n",
    "    if show_output_on_notebook:\n",
    "        print('\\nMutual_info_classification features:' ,repeated_mutual_features)\n",
    "        print('-'*100)\n",
    "\n",
    "    # Get the repeated features in correlation table \n",
    "    repeated_corr_features = []\n",
    "    corr_word_counts = collections.Counter(top_corr_features)\n",
    "\n",
    "    for corr_word, corr_count in sorted(corr_word_counts.items()):\n",
    "        if show_output_on_notebook:\n",
    "            print('\"%s\" is repeated %d time%s.' % (corr_word, corr_count, \"s\" if corr_count > 1 else \"\"))\n",
    "        if corr_count >= 3:\n",
    "            repeated_corr_features.append(corr_word)\n",
    "    if show_output_on_notebook:\n",
    "        print(\"\\nPearson's correlation features:\" ,repeated_corr_features)\n",
    "        print('-'*100)\n",
    "\n",
    "    # Extract the features for the chosen columns later in the model training phase \n",
    "    joined_features = repeated_corr_features + repeated_mutual_features\n",
    "    repeated_joined_features = []\n",
    "    joined_word_counts = collections.Counter(joined_features)\n",
    "\n",
    "    for joined_word, joined_count in sorted(joined_word_counts.items()):\n",
    "        if show_output_on_notebook:\n",
    "            print('\"%s\" is repeated %d time%s.' % (joined_word, joined_count, \"s\" if joined_count > 1 else \"\"))\n",
    "        if joined_count == 2:\n",
    "            repeated_joined_features.append(joined_word)\n",
    "\n",
    "    training_features = joined_features\n",
    "\n",
    "    for repeated in repeated_joined_features:\n",
    "        training_features.remove(repeated)\n",
    "    if show_output_on_notebook:\n",
    "        print('\\nFeatures that will be used in model training: ',training_features)\n",
    "        print('\\nTotal number of training features: ', len(training_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f920844",
   "metadata": {},
   "source": [
    "### In this part of the code, we will extract the X(Training Features) and y(Labels) by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0df7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QuestionSB == \"Question 4\"  or show_output_on_notebook:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.svm import SVR\n",
    "\n",
    "    random_seed = 9\n",
    "\n",
    "    q4_classifier_names = ['rf_class', 'knn', 'rf_reg', 'svr']\n",
    "    q4_classifiers = {}\n",
    "\n",
    "    rf_class_clf = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth = 5, random_state=random_seed, n_jobs=-1)\n",
    "    q4_classifiers['rf_class'] = rf_class_clf\n",
    "\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=15, weights='uniform', n_jobs = -1)\n",
    "    q4_classifiers['knn'] = knn_clf\n",
    "\n",
    "    rf_reg_clf = RandomForestRegressor(n_estimators=1000, criterion='mae', random_state=1, n_jobs=-1)\n",
    "    q4_classifiers['rf_reg'] = rf_reg_clf\n",
    "\n",
    "    svr_clf = SVR(kernel='poly', C=0.8)\n",
    "    q4_classifiers['svr'] = svr_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3c737",
   "metadata": {},
   "source": [
    "### Create a function that will plot the decision region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c24943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=1):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.xlim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6,\n",
    "                    c=cmap(idx), edgecolor='black', marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c681b",
   "metadata": {},
   "source": [
    "### After trying, we decide to leave the datasets as it is because it does not yield better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8bfc54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "if QuestionSB == \"Question 4\"  or show_output_on_notebook:\n",
    "    st.title(\"Classification Model's MAE Score For 4 States\")\n",
    "    if show_output_on_notebook:\n",
    "        print(\"rf_class = Random Forests Classifiers\")\n",
    "        print(\"knn = K-nearest Neighbors Algorithm\")\n",
    "        print(\"rf_reg = Random Forest Regression\")\n",
    "        print(\"svr = Support Vector Regression\")\n",
    "    st.markdown(\"rf_class = Random Forests Classifiers\")\n",
    "    st.markdown(\"knn = K-nearest Neighbors Algorithm\")\n",
    "    st.markdown(\"rf_reg = Random Forest Regression\")\n",
    "    st.markdown(\"svr = Support Vector Regression\")\n",
    "    q4_state_list = ['Pahang', 'Kedah', 'Johor', 'Selangor']\n",
    "    q4_combined_df = q3_combined_df\n",
    "    stdsc = StandardScaler()\n",
    "    mae_score_list = []\n",
    "    # Set random seed so that the results is reproducable in the future\n",
    "    random_seed = 9\n",
    "\n",
    "    for q4_states in q4_state_list:\n",
    "        q4_df = q4_combined_df[q4_states]\n",
    "#         q4_df_std = stdsc.fit_transform(q4_df)\n",
    "        y = q4_df['cases_new']\n",
    "#         X = q4_df.drop(columns=['state', 'cases_new','date'])\n",
    "        X = q4_df[training_features]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = random_seed)\n",
    "\n",
    "        # Standardization Scaling the training features\n",
    "    #     X_train_std = stdsc.fit_transform(X_train)\n",
    "    #     X_test_std = stdsc.fit_transform(X_test)\n",
    "    #     # Reshape 1D array into 2D array for transformation \n",
    "    #     y_train = y_train.values.reshape(len(y_train), 1)\n",
    "    #     y_test = y_test.values.reshape(len(y_test), 1)\n",
    "    #     y_train_std = stdsc.fit_transform(y_train)\n",
    "    #     y_test_std = stdsc.fit_transform(y_test)\n",
    "    #     # Reshape 2D array back into 1D array for model training \n",
    "    #     y_train = y_train.ravel()\n",
    "    #     y_test = y_test.ravel()\n",
    "    #     y_train_std = y_train_std.ravel()\n",
    "    #     y_test_std = y_test_std.ravel()\n",
    "\n",
    "        # Model Training \n",
    "        for classifiers in q4_classifier_names:\n",
    "            if show_output_on_notebook:\n",
    "                print('Currently Training '+classifiers+' model for ' + q4_states)\n",
    "            q4_classifiers[classifiers].fit(X_train, y_train)\n",
    "            # Evaluation Metrtics \n",
    "            y_train_pred = q4_classifiers[classifiers].predict(X_train)\n",
    "            y_test_pred = q4_classifiers[classifiers].predict(X_test)\n",
    "            mae_score = mean_absolute_error(y_test, y_test_pred)\n",
    "            mae_score_round = round(mae_score, 3)\n",
    "            if show_output_on_notebook:\n",
    "                print('The MAE score for '+classifiers+' is ', mae_score_round)\n",
    "                print('-'*100)\n",
    "            mae_score_list.append(mae_score_round)\n",
    "\n",
    "        # Plot the bar plot of the MAE scores of each model for comparison puposes\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        ax.bar(q4_classifier_names, mae_score_list)\n",
    "        ax.set_ylabel('MAE Score')\n",
    "        st.header(\"Performance of each model in \" + q4_states)\n",
    "        if show_output_on_notebook:\n",
    "            plt.show()\n",
    "            print('-'*100)\n",
    "        st.pyplot()\n",
    "        mae_score_list.clear()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fba0d",
   "metadata": {},
   "source": [
    "### For the last question which requires 2 techniques for each classification and regression models, I have used RandomForest Classification and KNearestNeighbor for classification, RandomForest Regression and SVR for regression. For the metric that is chosen to evaluate the performance of the model, I have decided to go with Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE), which is similar to MAE but gives us a more general and better feeling of how accurate is the forecast compared to the real data percentage wise.\n",
    "\n",
    "### In this case of predicting cases, we are essentially training a model to predictict a value based on the information that we fed in. So in theory, a regression model would seem to work better for this particular situation.\n",
    "\n",
    "### From the results, we could see that the RandomForest Regression model works best for predicting the daily cases for these states, reporting the lowest MAE and MAPE scores in all four states. We have tried standardizing the features to experiment if we could get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf9c2c8",
   "metadata": {
    "scrolled": true,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangl\\anaconda3\\lib\\site-packages\\traitlets\\traitlets.py:2196: FutureWarning: Supporting extra quotes around Unicode is deprecated in traitlets 5.0. Use 'remove_cell' instead of \"'remove_cell'\" – or use CUnicode.\n",
      "  warn(\n",
      "[NbConvertApp] Converting notebook Question3.ipynb to script\n",
      "[NbConvertApp] Writing 23085 bytes to Question3.py\n"
     ]
    }
   ],
   "source": [
    "# before use streamlit:\n",
    "# conda install -c conda-forge nbconvert\n",
    "# run this cell (for convert ipynb to py)\n",
    "# run this cmd in anaconda powershell prompt streamlit run .\\Question3.py (remember change path to this file's location)\n",
    "!jupyter nbconvert Question3.ipynb --to script --TagRemovePreprocessor.enabled=True --ClearMetadataPreprocessor.enabled=True -TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}'\n",
    "\n",
    "# !heroku create\n",
    "# git init\n",
    "# git add .\n",
    "# git commit -m \"initial commit\"\n",
    "# heroku create\n",
    "# git push heroku master"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
